# Twitter Data Collection
# Anna Berman


# SET UP ------------------------------------------------------------------
setwd('/Users/annaberman/Desktop/703 Text Analysis/FinalProject')

# library(devtools)
# install_github('andreacirilloac/updateR')
# library(updateR)
# updateR(admin_password = 'admin password')
# install.packages('rtweet')
# install.packages('ggplot2')
# install.packages('dplyr')
# install.packages('lubridate')
# install.packages('jsonlite')
# install.packages('maps')
# install.packages('tm')
# install.packages('slam')
# install.packages('topicmodels')
# install.packages('tidyverse')
# install.packages('tidytext')
# install.packages('rvest')

library(rtweet) # scraping tweets
library(ggplot2) # plots
library(dplyr) # tidyverse
library(lubridate) # easy date conversions
library(jsonlite) # read and write json files

#Credentialing
app_name<-"test_aeb100"
consumer_key<-"jz0yNGON4gdni3WdqPskhGcdq"
consumer_secret<-"RVMkCW1ZJfqJklgASFEDll9Z48IPB3zinYBZo1zdbnFRg6WpU2"
access_token<-"1041738207988203523-WnjOymlOT6RWSI6nMl4Z75XXAGw1jn"
access_token_secret<-"WCSD29t8btaJpS36FjzX1TNWBJY2nzjU3Xer5t9pVA4Ey"
create_token(app=app_name, consumer_key=consumer_key, consumer_secret=consumer_secret)


# DATA COLLECTION ---------------------------------------------------------

# Depression
# Seach_tweets function
# Pulling once a day
# The first day I pulled 'depression' and then rethought my approach
# I believe depressed is more likely to be associated with actually depressed people

# 11/20/18
depression_tweets1 <- search_tweets("depression",
                                   "lang:en", geocode = lookup_coords("usa"),
                                   n = 10000, type = "recent", include_rts = FALSE)


# 11/21/18
depression_tweets2 <- search_tweets("depressed",
                                   "lang:en", geocode = lookup_coords("usa"),
                                   n = 10000, type = "recent", include_rts = FALSE)

# 11/23/18
depression_tweets3 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 18000, type = "recent", include_rts = FALSE)

# 11/24/18
depression_tweets4 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 10000, type = "recent", include_rts = FALSE)

# 11/25/18
depression_tweets5 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 10000, type = "recent", include_rts = FALSE)

# 11/26/18
depression_tweets6 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 18000, type = "recent", include_rts = FALSE)

# 11/27/18
depression_tweets7 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 18000, type = "recent", include_rts = FALSE)

# 11/28/18
depression_tweets8 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 18000, type = "recent", include_rts = FALSE)

# 11/29/18
depression_tweets9 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 18000, type = "recent", include_rts = FALSE)

# 11/30/18
depression_tweets10 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 18000, type = "recent", include_rts = FALSE)

# 11/31/18
depression_tweets11 <- search_tweets("depressed",
                                     "lang:en", geocode = lookup_coords("usa"),
                                     n = 18000, type = "recent", include_rts = FALSE)


# Bind together new data
depression_tweets <- rbind(depression_tweets1, depression_tweets2)
depression_tweets <- rbind(depression_tweets, depression_tweets3)
depression_tweets <- rbind(depression_tweets, depression_tweets4)
depression_tweets <- rbind(depression_tweets, depression_tweets5)
depression_tweets <- rbind(depression_tweets, depression_tweets6)
depression_tweets <- rbind(depression_tweets, depression_tweets7)
depression_tweets <- rbind(depression_tweets, depression_tweets8)
depression_tweets <- rbind(depression_tweets, depression_tweets9)
depression_tweets <- rbind(depression_tweets, depression_tweets10)
depression_tweets <- rbind(depression_tweets, depression_tweets11)
depression_tweets <- depression_tweets[-duplicated(depression_tweets)]


# Create standardized data table
standard <- data.frame(id = seq(nrow(depression_tweets))) %>%
    mutate(keyword = 'depressed',
           platform = 'Twitter',
           created_at = depression_tweets$created_at,
           title = NA,
           content = depression_tweets$text,
           author = depression_tweets$screen_name,
           url = depression_tweets$url,
           likes = depression_tweets$favorite_count,
           views = NA,
           tags = depression_tweets$hashtags)

# Label the first keyword as 'depression' as opposed to 'depressed'
standard[seq(17872), 'keyword'] <- 'depresssion'


# Export as jsons
write_json(depression_tweets1, './Raw Tweets/depression_tweets_181120.json')
write_json(depression_tweets2, './Raw Tweets/depression_tweets_181121.json')
write_json(depression_tweets3, './Raw Tweets/depression_tweets_181123.json')
write_json(depression_tweets4, './Raw Tweets/depression_tweets_181124.json')
write_json(depression_tweets5, './Raw Tweets/depression_tweets_181125.json')
write_json(depression_tweets6, './Raw Tweets/depression_tweets_181126.json')
write_json(depression_tweets7, './Raw Tweets/depression_tweets_181127.json')
write_json(depression_tweets8, './Raw Tweets/depression_tweets_181128.json')
write_json(depression_tweets9, './Raw Tweets/depression_tweets_181129.json')
write_json(depression_tweets10, './Raw Tweets/depression_tweets_181130.json')
write_json(depression_tweets11, './Raw Tweets/depression_tweets_181131.json')
write_json(depression_tweets, './Raw Tweets/depression_tweets_all.json')
write_json(standard, './Raw Tweets/standard.json')

# Random Sample
# 11/2/18
random_sample1 <- search_tweets('lang:en', geocode = lookup_coords("usa"),
                                n = 18000, type = "recent", include_rts = FALSE)

# 11/4/18
random_sample2 <- search_tweets('lang:en', geocode = lookup_coords("usa"),
                                n = 18000, type = "recent", include_rts = FALSE)

# 11/5/18
random_sample3 <- search_tweets('lang:en', geocode = lookup_coords("usa"),
                                n = 18000, type = "recent", include_rts = FALSE)


?search_tweets
# 11/6/18
random_sample4 <- search_tweets('lang:en', geocode = lookup_coords("usa"),
                                n = 18000, type = "recent", include_rts = FALSE) %>%
    select(names(random_tweets))

# 11/7/18
random_sample4 <- search_tweets('lang:en', geocode = lookup_coords("usa"),
                                n = 18000, type = "recent", include_rts = FALSE)

# 11/7/18
# Try mixed, see if that changes the frequency of the data collection
random_sample5 <- search_tweets('lang:en', geocode = lookup_coords("usa"),
                                n = 18000, type = "mixed", include_rts = FALSE)

# Bind together new data
random_tweets <- rbind(random_sample1, random_sample2)
random_tweets <- rbind(random_tweets, random_sample3)
random_tweets <- rbind(random_tweets, random_sample4)


# Create standardized data table
random_standard <- data.frame(id = seq(nrow(random_tweets))) %>%
    mutate(keyword = 'random',
           platform = 'Twitter',
           created_at = random_tweets$created_at,
           title = NA,
           content = random_tweets$text,
           author = random_tweets$screen_name,
           url = random_tweets$url,
           likes = random_tweets$favorite_count,
           views = NA,
           tags = random_tweets$hashtags)

write_json(random_sample1, './Raw Tweets/random_tweets181102.json')
write_json(random_sample2, './Raw Tweets/random_tweets181104.json')
write_json(random_sample3, './Raw Tweets/random_tweets181105.json')
write_json(random_sample4, './Raw Tweets/random_tweets181106.json')
write_json(random_tweets, './Raw Tweets/random_tweets_all.json')
write_json(random_standard, './Raw Tweets/random_standard.json')

# DESCRIPTIVE STATS -------------------------------------------------------

# Load in full datasets
depression_tweets <- fromJSON('./Raw Tweets/depression_tweets_all.json')
random_tweets <- fromJSON('./Raw Tweets/random_tweets_all.json')
standard <- fromJSON('./Raw Tweets/standard.json')
random_standard <- fromJSON('./Raw Tweets/random_standard.json')


# Make sure createdat are dates
random_standard$created_at <- as.POSIXct(random_standard$created_at, 
                                         format = "%Y-%m-%d %H:%M:%S")

# Depressed
# Plot time series of frequency of depressed tweets
ts_plot(depression_tweets, "30 minutes") + 
    ggplot2::theme_minimal() +
    ggplot2::theme(plot.title = ggplot2::element_text(face="bold")) +
    ggplot2::labs(
        x = NULL, y = NULL,
        title = "Frequency of Tweets about Depression from the Past Week",
        subtitle = "Twitter status (tweet) counts aggregated using 30 minute intervals",
        caption = "\nSource: Data collected from Twitter's REST API via rtweet")

# Random
#Plot time series of frequency of random tweets
ts_plot(random_tweets, "30 minutes") + 
    ggplot2::theme_minimal() +
    ggplot2::theme(plot.title = ggplot2::element_text(face="bold")) +
    ggplot2::labs(
        x = NULL, y = NULL,
        title = "Frequency of Random Tweets from the Past Week",
        subtitle = "Twitter status (tweet) counts aggregated using 30 minute intervals",
        caption = "\nSource: Data collected from Twitter's REST API via rtweet")

# Depressed
# Plot frequency of depressed tweets by the hour
standard %>%
    # Create temporary time_of_day variable 
    mutate(time_of_day = hour(created_at) + minute(created_at)/60) %>%
    ggplot() + 
    # Plot in 10 minutes intervals
    geom_histogram(mapping = aes(time_of_day), bins = 144, 
                   fill = 'skyblue3', alpha = .8) +
    theme_minimal() +
    theme(plot.title = element_text(face = 'bold')) +
    labs(title = 'Frequency of \'Depressed\' Tweets by the Hour', 
         subtitle = 'Tweet counts aggregated by time of day',
         caption = "Source: Data collected from Twitter's REST API via rtweet",
         x = 'Time of Day (10 minute intervals, military time)',
         y = 'Frequency of Tweets') +
    xlim(c(0,24))


# Random
# Plot frequency of random tweets by the hour
# The issue is that there are so many random tweets that I just
# get all of them at once
random_standard %>%
    # Create temporary time_of_day variable 
    mutate(time_of_day = hour(created_at) + minute(created_at)/60) %>%
    ggplot() + 
    # Plot in 10 minutes intervals
    geom_histogram(mapping = aes(time_of_day), bins = 144, 
                   fill = 'skyblue3', alpha = .8) + 
    theme_minimal() +
    theme(plot.title = element_text(face = 'bold')) + 
    labs(title = 'Frequency of Random Tweets by the Hour', 
         subtitle = 'Tweet counts aggregated by time of day',
         caption = "Source: Data collected from Twitter's REST API via rtweet",
         x = 'Time of Day (10 minute intervals, military time)',
         y = 'Frequency of Tweets') +
    xlim(c(0,24))


# Need to make sure you do a balanced sample
# Also need to make sure you correctly label...
# Plot frequency of tweets by Weekday
standard %>% 
    mutate(weekday = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", 
                       "Friday", "Saturday")[wday(created_at)],
           weekday = factor(weekday, level = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", 
                                               "Friday", "Saturday"))) %>%
    ggplot() + 
    # Plot by day
    geom_bar(mapping = aes(weekday), fill = 'skyblue3', alpha = .8) +
    theme_minimal() +
    theme(plot.title = element_text(face = 'bold')) +
    labs(title = 'Frequency of \'Depressed\' Tweets by the Hour', 
         subtitle = 'Tweet counts aggregated by time of day',
         caption = "Source: Data collected from Twitter's REST API via rtweet",
         x = 'Time of Day (5 minute intervals, military time)',
         y = 'Frequency of Tweets') 

# Plot location of tweets
# Geocode tweets
dep_geocoded <- lat_lng(depression_tweets)
ran_geocoded <- lat_lng(random_tweets)
# How many tweets have location services turned on
dep_geo_num <- sum(!is.na(dep_geocoded$lat))
ran_geo_num <- sum(!is.na(ran_geocoded$lat))
# Percentage of tweets with location servicees turned on
dep_geo_stat <- round(dep_geo_num/nrow(dep_geocoded)*100,1)
ran_geo_stat <- round(ran_geo_num/nrow(ran_geocoded)*100,1)


# Plot on a map of US
library(maps)
par(mar = c(0,0,.25,0),
    mfrow = c(2,1))
maps::map("state", lwd = .25)
with(dep_geocoded, points(lng, lat, pch = 20, cex = .2, col = 'skyblue3'))
maps::map("state", lwd = .25)
with(ran_geocoded, points(lng, lat, pch = 20, cex = .2, col = 'red'))
title(main = paste('Geocoded Depression Tweets \n(',  dep_geo_num, ', ', dep_geo_stat , '%)', 
                   sep = ''),line = 1, cex =.5)

# Frequent depressed posters
depression_tweets %>%
    group_by(screen_name) %>%
    summarize(n = n()) %>%
    arrange(desc(n))


# TOPIC MODEL ---------------------------------------------------------------------

# Create DTM 
library(tm)
library(slam)
depression_corpus <- Corpus(VectorSource(as.vector(depression_tweets$text)))
depression_corpus = tm_map(depression_corpus, removeWords, append(stopwords('english'), c('https', 'co', 'amp', 'the'))) #remove stopwords
depression_DTM <- DocumentTermMatrix(depression_corpus, control = list(wordLengths = c(2, Inf)))
rowTotals <- slam::row_sums(depression_DTM, na.rm = T) #Find the sum of words in each Document
#rowTotals <- apply(depression_DTM , 1, sum) #Find the sum of words in each Document
depression_DTM   <- depression_DTM[rowTotals> 0, ]           #remove all docs without words


# Topic Modeling
library(topicmodels)
library(tidyverse)
library(tidytext)
library(rvest)

# Run a topic model with 5 topics
topic_model<-LDA(depression_DTM, k=3, control = list(seed = 321)) # k = # of topics in corpus

DEP_topics <- tidytext::tidy(topic_model, matrix = "beta")



# Get the top 10 most frequent words in each topic
dep_top_terms <- 
    DEP_topics %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)

# Plot the topics
dep_top_terms %>%
    mutate(term = reorder(term, beta)) %>%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip()



# Run a topic model with 10 topics
topic_model <- LDA(depression_DTM, k=6, control = list(seed = 321)) # k = # of topics in corpus
topic_model_td <- tidy(topic_model)



# Get the top 10 most frequent words in each topic
dep_top_terms <- 
    topid_model_td  %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)



# NETWORK ANALYSIS --------------------------------------------------------

#install.packages('devtools')
library(devtools)
#install_github("cbail/textnets")
library(textnets)

# Depressed words
# Pre-process text network
prepped_depressed <- PrepText(standard, groupvar = 'id', 
                              textvar = "content", node_type = "words", 
                              tokenizer = "words", pos = "nouns", 
                              remove_stop_words = TRUE, 
                              compound_nouns = TRUE)

# Create Text Network
depressed_text_network <- CreateTextnet(prepped_depressed)

# Viz
VisTextNet(depressed_text_network, label_degree_cut = 0)
VisTextNetD3(depressed_text_network)


# WORD2VEC ----------------------------------------------------------------
library(tidytext)
#install.packages('widyr')
library(widyr)
library(tidyr)
#install.packages('irlba')
library(irlba)
install.packages('broom')
library(broom)

dep_tweets <- fromJSON('/Users/annaberman/Desktop/703 Text Analysis/FinalProject/Raw Tweets/depression_tweets_181120.json')

# Take out retweets 
dep_tweets <- dep_tweets %>%
    filter(is_retweet == F) %>%
    select(text)

# Create tweet id
dep_tweets$postID <- row.names(dep_tweets)

# Calculate probs (containing stopwords)
unigram_probs <- dep_tweets %>%
    unnest_tokens(word, text) %>%
    count(word, sort = TRUE) %>%
    mutate(p = n/sum(n))
unigram_probs

# Defining context window
tidy_skipgrams <- dep_tweets %>%
    unnest_tokens(ngram, text, token = 'ngrams', n=8) %>%
    mutate(ngramID = row_number()) %>%
    unite(skipgramID, postID, ngramID) %>%
    unnest_tokens(word, ngram)

# Calculate probs of words being near each other
skipgram_probs <- tidy_skipgrams %>%
    pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
    mutate(p = n/sum(n))

# Normalized probability
normalized_prob <- skipgram_probs %>%
    filter(n > 20) %>%
    rename(word1 = item1, word2 = item2) %>%
    left_join(unigram_probs %>%
                  select(word1 = word, p1 = p),
              by = "word1") %>%
    left_join(unigram_probs %>%
                  select(word2 = word, p2 = p),
              by = "word2") %>%
    mutate(p_together = p / p1 / p2)

# Words most associated with depressed
normalized_prob %>%
    filter(word1 == 'depressed') %>%
    arrange(-p_together)

# Matrix Factorization
pmi_matrix <- normalized_prob %>% 
    mutate(pmi = log10(p_together)) %>% 
    cast_sparse(word1, word2, pmi)


# Reduce dimensionality
#remove missing data 
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0 
#next we run SVD
pmi_svd <- irlba(pmi_matrix, 256, maxit = 500)
#next we output the word vectors: 
word_vectors <- pmi_svd$u 
rownames(word_vectors) <- rownames(pmi_matrix)

# Find synonyms
search_synonyms <- function(word_vectors, selected_vector) {
    similarities <- word_vectors %*% selected_vector %>%
        tidy() %>%
        as_tibble() %>% 
        rename(token = .rownames, similarity = unrowname.x.)
    similarities %>% arrange(-similarity)
}

depressed <- search_synonyms(word_vectors, word_vectors['depressed',])
depressed
