# Twitter Data Collection
# Anna Berman


# SET UP ------------------------------------------------------------------
setwd('/Users/annaberman/Desktop/703 Text Analysis/FinalProject')

library(rtweet) # scraping tweets
library(ggplot2) # plots
library(dplyr) # tidyverse
library(lubridate) # easy date conversions
library(jsonlite) # read and write json files

#Credentialing
app_name<-"test_aeb100"
consumer_key<-"jz0yNGON4gdni3WdqPskhGcdq"
consumer_secret<-"RVMkCW1ZJfqJklgASFEDll9Z48IPB3zinYBZo1zdbnFRg6WpU2"
access_token<-"1041738207988203523-WnjOymlOT6RWSI6nMl4Z75XXAGw1jn"
access_token_secret<-"WCSD29t8btaJpS36FjzX1TNWBJY2nzjU3Xer5t9pVA4Ey"
create_token(app=app_name, consumer_key=consumer_key, consumer_secret=consumer_secret)



# DATA COLLECTION ---------------------------------------------------------

# Depression
# Seach_tweets function
# Pulling once a day
# The first day I pulled 'depression' and then rethought my approach
# I believe depressed is more likely to be associated with actually depressed people

# 11/20/18
depression_tweets1 <- search_tweets("depression",
                                   "lang:en", geocode = lookup_coords("usa"),
                                   n = 10000, type = "recent", include_rts = FALSE)


# 11/21/18
depression_tweets2 <- search_tweets("depressed",
                                   "lang:en", geocode = lookup_coords("usa"),
                                   n = 10000, type = "recent", include_rts = FALSE)

# 11/23/18
depression_tweets3 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 18000, type = "recent", include_rts = FALSE)

# 11/24/18
depression_tweets4 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 10000, type = "recent", include_rts = FALSE)

# 11/25/18
depression_tweets5 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 10000, type = "recent", include_rts = FALSE)

# 11/26/18
depression_tweets6 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 18000, type = "recent", include_rts = FALSE)

# 11/27/18
depression_tweets7 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 18000, type = "recent", include_rts = FALSE)

# 11/28/18
depression_tweets8 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 18000, type = "recent", include_rts = FALSE)

# 11/29/18
depression_tweets9 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 18000, type = "recent", include_rts = FALSE)

# 11/30/18
depression_tweets10 <- search_tweets("depressed",
                                    "lang:en", geocode = lookup_coords("usa"),
                                    n = 18000, type = "recent", include_rts = FALSE)

# 11/31/18
depression_tweets11 <- search_tweets("depressed",
                                     "lang:en", geocode = lookup_coords("usa"),
                                     n = 18000, type = "recent", include_rts = FALSE)


# Bind together new data
depression_tweets <- rbind(depression_tweets1, depression_tweets2)
depression_tweets <- rbind(depression_tweets, depression_tweets3)
depression_tweets <- rbind(depression_tweets, depression_tweets4)
depression_tweets <- rbind(depression_tweets, depression_tweets5)
depression_tweets <- rbind(depression_tweets, depression_tweets6)
depression_tweets <- rbind(depression_tweets, depression_tweets7)
depression_tweets <- rbind(depression_tweets, depression_tweets8)
depression_tweets <- rbind(depression_tweets, depression_tweets9)
depression_tweets <- rbind(depression_tweets, depression_tweets10)
depression_tweets <- rbind(depression_tweets, depression_tweets11)
depression_tweets <- depression_tweets[-duplicated(depression_tweets)]


# Create standardized data table
standard <- data.frame(id = seq(nrow(depression_tweets))) %>%
    mutate(keyword = 'depressed',
           platform = 'Twitter',
           created_at = depression_tweets$created_at,
           title = NA,
           content = depression_tweets$text,
           author = depression_tweets$screen_name,
           url = depression_tweets$url,
           likes = depression_tweets$favorite_count,
           views = NA,
           tags = depression_tweets$hashtags)

# Label the first keyword as 'depression' as opposed to 'depressed'
standard[seq(17872), 'keyword'] <- 'depresssion'


# Export as jsons
write_json(depression_tweets1, './Raw Tweets/depression_tweets_181120.json')
write_json(depression_tweets2, './Raw Tweets/depression_tweets_181121.json')
write_json(depression_tweets3, './Raw Tweets/depression_tweets_181123.json')
write_json(depression_tweets4, './Raw Tweets/depression_tweets_181124.json')
write_json(depression_tweets5, './Raw Tweets/depression_tweets_181125.json')
write_json(depression_tweets6, './Raw Tweets/depression_tweets_181126.json')
write_json(depression_tweets7, './Raw Tweets/depression_tweets_181127.json')
write_json(depression_tweets8, './Raw Tweets/depression_tweets_181128.json')
write_json(depression_tweets9, './Raw Tweets/depression_tweets_181129.json')
write_json(depression_tweets10, './Raw Tweets/depression_tweets_181130.json')
write_json(depression_tweets11, './Raw Tweets/depression_tweets_181131.json')
write_json(depression_tweets, './Raw Tweets/depression_tweets_all.json')
write_json(standard, './Raw Tweets/standard.json')


# Random Sample
# 11/2/18
random_sample1 <- search_tweets('lang:en', geocode = lookup_coords("usa"),
                                n = 18000, type = "recent", include_rts = FALSE)

# 11/4/18
random_sample2 <- search_tweets('lang:en', geocode = lookup_coords("usa"),
                                n = 18000, type = "recent", include_rts = FALSE)

# Bind together new data
random_tweets <- rbind(random_sample1, random_sample2)

# Create standardized data table
random_standard <- data.frame(id = seq(nrow(random_tweets))) %>%
    mutate(keyword = 'random',
           platform = 'Twitter',
           created_at = random_tweets$created_at,
           title = NA,
           content = random_tweets$text,
           author = random_tweets$screen_name,
           url = random_tweets$url,
           likes = random_tweets$favorite_count,
           views = NA,
           tags = random_tweets$hashtags)

write_json(random_sample1, './Raw Tweets/random_tweets181102.json')
write_json(random_sample2, './Raw Tweets/random_tweets181104.json')
write_json(random_tweets, './Raw Tweets/random_tweets_all.json')
write_json(random_standard, './Raw Tweets/random_standard.json')


# DESCRIPTIVE STATS -------------------------------------------------------

# Plot time series of frequency of tweets
ts_plot(depression_tweets, "30 minutes") + 
    ggplot2::theme_minimal() +
    ggplot2::theme(plot.title = ggplot2::element_text(face="bold")) +
    ggplot2::labs(
        x = NULL, y = NULL,
        title = "Frequency of Tweets about Depression from the Past Week",
        subtitle = "Twitter status (tweet) counts aggregated using 30 minute intervals",
        caption = "\nSource: Data collected from Twitter's REST API via rtweet"
    )


# Plot frequency of tweets by the hour
standard %>%
    # Create temporary time_of_day variable 
    mutate(time_of_day = hour(created_at) + minute(created_at)/60) %>%
    ggplot() + 
    # Plot in 5 minutes intervals
    geom_histogram(mapping = aes(time_of_day), bins = 144, 
                   fill = 'skyblue3', alpha = .8) +
    theme_minimal() +
    theme(plot.title = element_text(face = 'bold')) +
    labs(title = 'Frequency of \'Depressed\' Tweets by the Hour', 
         subtitle = 'Tweet counts aggregated by time of day',
         caption = "Source: Data collected from Twitter's REST API via rtweet",
         x = 'Time of Day (10 minute intervals, military time)',
         y = 'Frequency of Tweets') +
    xlim(c(0,24))


# Need to make sure you do a balanced sample
# Also need to make sure you correctly label...
# Plot frequency of tweets by Weekday
standard %>% 
    mutate(weekday = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", 
                       "Friday", "Saturday")[wday(created_at)],
           weekday = factor(weekday, level = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", 
                                               "Friday", "Saturday"))) %>%
    ggplot() + 
    # Plot by day
    geom_bar(mapping = aes(weekday), fill = 'skyblue3', alpha = .8) +
    theme_minimal() +
    theme(plot.title = element_text(face = 'bold')) +
    labs(title = 'Frequency of \'Depressed\' Tweets by the Hour', 
         subtitle = 'Tweet counts aggregated by time of day',
         caption = "Source: Data collected from Twitter's REST API via rtweet",
         x = 'Time of Day (5 minute intervals, military time)',
         y = 'Frequency of Tweets') 

# Plot location of tweets
# Geocode tweets
dep_geocoded <- lat_lng(depression_tweets)
ran_geocoded <- lat_lng(random_sample1)
# How many tweets have location services turned on
dep_geo_num <- sum(!is.na(dep_geocoded$lat))
ran_geo_num <- sum(!is.na(ran_geocoded$lat))
# Percentage of tweets with location servicees turned on
dep_geo_stat <- round(geo_num/nrow(geocoded)*100,1)
ran_geo_stat <- round(geo_num/nrow(geocoded)*100,1)


# Plot on a map of US
library(maps)
par(mar = c(0,0,.25,0),
    mfrow = c(2,1))
maps::map("state", lwd = .25)
with(dep_geocoded, points(lng, lat, pch = 20, cex = .2, col = 'skyblue3'))
maps::map("state", lwd = .25)
with(ran_geocoded, points(lng, lat, pch = 20, cex = .2, col = 'red'))
title(main = paste('Geocoded Depression Tweets \n(',  dep_geo_num, ', ', dep_geo_stat , '%)', 
                   sep = ''),line = 1, cex =.5)


# TOPIC MODEL ---------------------------------------------------------------------

# Create DTM 
library(tm)
depression_corpus <- Corpus(VectorSource(as.vector(depression_tweets$text)))
depression_corpus = tm_map(depression_corpus, removeWords, append(stopwords('english'), c('https', 'co', 'amp', 'the'))) #remove stopwords
depression_DTM <- DocumentTermMatrix(depression_corpus, control = list(wordLengths = c(2, Inf)))
rowTotals <- apply(depression_DTM , 1, sum) #Find the sum of words in each Document
depression_DTM   <- depression_DTM[rowTotals> 0, ]           #remove all docs without words


# Topic Modeling
library(topicmodels)
library(tidyverse)
library(tidytext)
library(rvest)

# Run a topic model with 5 topics
topic_model<-LDA(depression_DTM, k=5, control = list(seed = 321)) # k = # of topics in corpus

DEP_topics <- tidytext::tidy(topic_model, matrix = "beta")



# Get the top 10 most frequent words in each topic
dep_top_terms <- 
    DEP_topics %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)

# Plot the topics
dep_top_terms %>%
    mutate(term = reorder(term, beta)) %>%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip()



# Run a topic model with 10 topics
topic_model <- LDA(depression_DTM, k=6, control = list(seed = 321)) # k = # of topics in corpus
topic_model_td <- tidy(topic_model)



# Get the top 10 most frequent words in each topic
dep_top_terms <- 
    topid_model_td  %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)


